{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b655f51",
   "metadata": {},
   "source": [
    "# BottleCap Color Sorting with YOLOv11pico\n",
    "\n",
    "![Banner](../assets/banner.png)\n",
    "\n",
    "## Project Background\n",
    "Effective waste management, specifically plastic recycling, is a major global challenge. Separating bottle caps from bottles is a critical step because they are often made of different plastics (e.g., PP vs. PET). Manual sorting is slow and expensive, while industrial machines are often too large for smaller facilities.\n",
    "\n",
    "## Introduction\n",
    "The goal of this project is to build a **high-speed computer vision system** capable of detecting and classifying bottle caps into three categories: **Light Blue**, **Dark Blue**, and **Others**.\n",
    "\n",
    "Hardware Constraint: The system is designed to run on an edge device, specifically a **Raspberry Pi 5 equipped with an AI Accelerator (e.g., Hailo-8L or Coral TPU)**. To ensure smooth integration with mechanical sorters (like air jets or robotic arms), the model must achieve an inference latency of **5-10ms** per frame.\n",
    "\n",
    "### ⚠️ Important Note on Environment\n",
    "To achieve the strict latency requirement, I developed a custom, smaller version of YOLOv11 called **YOLOv11p (Pico)**.\n",
    "* **Repo:** This notebook runs inside a cloned and modified version of the Ultralytics repository.\n",
    "* **Modifications:** I modified `ultralytics/nn/tasks.py` to support a new scaling factor **Pico** (`p`) which is smaller than the standard **Nano** (`n`) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a69a98fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the parent directory (project root)\n",
    "project_root = os.path.abspath('..')\n",
    "\n",
    "# Add it to sys.path if it's not already there\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "\n",
    "import wandb\n",
    "from torch.cuda import empty_cache\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ac5b0e",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "First, we set up the project paths. Since this notebook sits inside the repository structure, we need to ensure the system path sees the root directory. We also configure Ultralytics to store datasets and training artifacts in specific folders to keep the project organized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e3fa264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import settings as ultralytics_settings\n",
    "ultralytics_settings.update({\n",
    "    'datasets_dir': os.path.join(project_root, 'datasets'),\n",
    "    'weights_dir': os.path.join(project_root, 'weights'),\n",
    "    'runs_dir': os.path.join(project_root, 'runs'),\n",
    "    'mlflow': False,\n",
    "    'wandb': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "965ce067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bsort.utils import (\n",
    "    extract_and_prepare_dataset, \n",
    "    download_from_roboflow,\n",
    "    print_metrics,\n",
    "    log_metrics_to_wandb\n",
    ")\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f985043",
   "metadata": {},
   "source": [
    "## 2. Experiment Configuration\n",
    "Here, I define the project constants. I will run multiple experiments:\n",
    "1.  **Baseline S & N:** Standard YOLO models (with COCO pretrained weights) to establish a performance benchmark.\n",
    "2.  **Baseline P:** My custom \"Pico\" model trained from scratch (random weights).\n",
    "3.  **Pretrain P:** The Pico model pretrained on a public dataset to learn the shape of bottle caps.\n",
    "4.  **Finetune P:** The final model, transfer-learned from the public dataset to the specific challenge dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12b1301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"bottle_cap_project_temp\"\n",
    "BASELINE_S_RUN_NAME = \"baseline_yolo11s\"\n",
    "BASELINE_N_RUN_NAME = \"baseline_yolo11n\"\n",
    "BASELINE_P_RUN_NAME = \"baseline_yolo11p\"\n",
    "PRETRAIN_P_RUN_NAME = \"pretrain_yolo11p\"\n",
    "FINE_TUNE_P_RUN_NAME = \"finetune_yolo11p\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182743c1",
   "metadata": {},
   "source": [
    "## 3. Data Preparation Strategy\n",
    "\n",
    "### The \"Cold Start\" Problem\n",
    "Standard YOLO models (N, S, M) come pretrained on the COCO dataset, which gives them a good understanding of general features. However, my custom **YOLOv11p** is a new architecture, so it has no pretrained weights.\n",
    "\n",
    "If I train the Pico model directly on the provided small sample dataset (only ~20 images), it will likely fail to generalize.\n",
    "\n",
    "**Solution:**\n",
    "I will use a **Public Dataset** from Roboflow containing generic bottle caps to pretrain the Pico model. This allows the model to learn what a \"bottle cap\" looks like before we teach it to distinguish the specific colors.\n",
    "\n",
    "<a href=\"https://universe.roboflow.com/work3-dqzz5/bottle-cap-y6pzg\">CLICK HERE FOR PUBLIC DATASET PAGE</a>\n",
    "\n",
    "![Public Dataset Example](../assets/public_dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a63724a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'workspace'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      2\u001b[39m public_config_path = target_location / \u001b[33m\"\u001b[39m\u001b[33mdata.yaml\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m roboflow_config = {\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mroboflow\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdataset_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mpublic\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     }\n\u001b[32m     13\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mdownload_from_roboflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroboflow_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_location\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\MUHAMMADRAFLYARJASUB\\Documents\\ultralytics\\bsort\\utils\\data.py:93\u001b[39m, in \u001b[36mdownload_from_roboflow\u001b[39m\u001b[34m(rf_config, target_dir)\u001b[39m\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mROBOFLOW_API_KEY not set.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     92\u001b[39m rf = Roboflow(api_key=api_key)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m project = rf.workspace(\u001b[43mrf_config\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mworkspace\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m).project(rf_config[\u001b[33m\"\u001b[39m\u001b[33mproject\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     94\u001b[39m version = project.version(rf_config[\u001b[33m\"\u001b[39m\u001b[33mversion\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     96\u001b[39m version.download(\u001b[33m\"\u001b[39m\u001b[33myolov11\u001b[39m\u001b[33m\"\u001b[39m, location=\u001b[38;5;28mstr\u001b[39m(target_dir))\n",
      "\u001b[31mKeyError\u001b[39m: 'workspace'"
     ]
    }
   ],
   "source": [
    "target_location = Path(\"..\") / \"datasets\" / \"processed\" / \"public\"\n",
    "public_config_path = target_location / \"data.yaml\"\n",
    "\n",
    "roboflow_config = {\n",
    "    \"workspace\": \"work3-dqzz5\",\n",
    "    \"project\": \"bottle-cap-y6pzg\",\n",
    "    \"version\": 1\n",
    "}\n",
    "\n",
    "download_from_roboflow(roboflow_config[\"roboflow\"], target_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc2cbd9",
   "metadata": {},
   "source": [
    "### Preparing the Challenge Dataset\n",
    "The provided sample dataset is very small. To ensure consistent evaluation, I decided not to use a random split. Instead, I manually separated the images into **Train** and **Validation** sets.\n",
    "\n",
    "I created a utility function `extract_and_prepare_dataset` that:\n",
    "1.  Extracts the raw images.\n",
    "2.  Splits them based on a hardcoded list (to keep classes balanced).\n",
    "3.  **Relabels the data:** The original labels are generic. I used a script to check the filename/folder and assign the correct class ID: `0: Others`, `1: Light Blue`, `2: Dark Blue`.\n",
    "\n",
    "![Sample Dataset Example](../assets/sample_dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e0d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_path = Path(\"..\") / \"datasets\" / \"sample.zip\"\n",
    "dataset_root_path = Path(\"..\") / \"datasets\"\n",
    "\n",
    "sample_config_path = extract_and_prepare_dataset(\n",
    "    zip_path=zip_file_path,\n",
    "    root_dir=dataset_root_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0356ea",
   "metadata": {},
   "source": [
    "## 4. Establishing Benchmarks\n",
    "\n",
    "### Experiment A: YOLOv11s (Small)\n",
    "I start by training the **YOLOv11s** model. This model serves as the \"Gold Standard\" for accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eebac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project=PROJECT_NAME, name=BASELINE_S_RUN_NAME) as run:    \n",
    "    model_baseline_s = YOLO(\"yolo11s.pt\")\n",
    "    \n",
    "    results_baseline_s = model_baseline_s.train(\n",
    "        data=str(sample_config_path),\n",
    "        project=PROJECT_NAME,\n",
    "        name=BASELINE_S_RUN_NAME,\n",
    "        epochs=150,\n",
    "        verbose=True,\n",
    "\n",
    "        hsv_h=0.0,\n",
    "        hsv_s=0.25,\n",
    "        hsv_v=0.25,\n",
    "        mixup=0.0,\n",
    "        optimizer=\"SGD\",\n",
    "        lr0=0.02,\n",
    "        weight_decay=0.01,\n",
    "        fliplr=0.5,\n",
    "        mosaic=0.25\n",
    "    )\n",
    "    \n",
    "    log_metrics_to_wandb(\n",
    "        results_baseline_s, \n",
    "        run_id=run.id, \n",
    "        project_name=PROJECT_NAME\n",
    "    )\n",
    "    \n",
    "    del model_baseline_s\n",
    "    gc.collect()\n",
    "    empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e9378b",
   "metadata": {},
   "source": [
    "### Result Analysis Baseline YOLOv11s\n",
    "The Small model achieved a high **mAP @ 50-95 of 0.9172** and a **Recall of 0.97**. This confirms that the dataset is high-quality and solvable. This score sets the upper bound of what is possible with this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b95452",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline YOLO11s Result on Sample Dataset:\")\n",
    "print_metrics(results_baseline_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88a39fb",
   "metadata": {},
   "source": [
    "### Experiment B: YOLOv11n (Nano)\n",
    "Next, I train the **YOLOv11n**. This is the smallest standard model provided by Ultralytics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b8397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project=PROJECT_NAME, name=BASELINE_N_RUN_NAME) as run:    \n",
    "    model_baseline_n = YOLO(\"yolo11n.pt\")\n",
    "    \n",
    "    results_baseline_n = model_baseline_n.train(\n",
    "        data=str(sample_config_path),\n",
    "        project=PROJECT_NAME,\n",
    "        name=BASELINE_N_RUN_NAME,\n",
    "        epochs=150,\n",
    "        verbose=True,\n",
    "\n",
    "        hsv_h=0.0,\n",
    "        hsv_s=0.25,\n",
    "        hsv_v=0.25,\n",
    "        mixup=0.0,\n",
    "        optimizer=\"SGD\",\n",
    "        lr0=0.02,\n",
    "        weight_decay=0.01,\n",
    "        fliplr=0.5,\n",
    "        mosaic=0.25\n",
    "    )\n",
    "    \n",
    "    log_metrics_to_wandb(\n",
    "        results_baseline_n, \n",
    "        run_id=run.id, \n",
    "        project_name=PROJECT_NAME\n",
    "    )\n",
    "    \n",
    "    del model_baseline_n\n",
    "    gc.collect()\n",
    "    empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797cdb41",
   "metadata": {},
   "source": [
    "### Result Analysis Baseline YOLOv11n\n",
    "The Nano model performed impressively, achieving an **mAP @ 50-95 of 0.9018**. This is only ~1.5% lower than the significantly larger \"Small\" model. It suggests that a lighter architecture can still handle this task effectively without a major drop in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6004b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline YOLO11n Result on Sample Dataset:\")\n",
    "print_metrics(results_baseline_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3695017c",
   "metadata": {},
   "source": [
    "## 5. The \"Pico\" Model (YOLOv11p)\n",
    "\n",
    "To strictly meet the **5-10ms** inference time on the edge device, I use the custom `yolov11p.yaml` (Pico) definition (Scale: 0.25, 0.25)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c23888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project=PROJECT_NAME, name=BASELINE_P_RUN_NAME) as run:\n",
    "    model_baseline_p = YOLO(\"yolo11p.yaml\")\n",
    "    \n",
    "    results_baseline_p = model_baseline_p.train(\n",
    "        data=str(sample_config_path),\n",
    "        project=PROJECT_NAME,\n",
    "        name=BASELINE_P_RUN_NAME,\n",
    "        epochs=150,\n",
    "        verbose=True,\n",
    "\n",
    "        hsv_h=0.0,\n",
    "        hsv_s=0.25,\n",
    "        hsv_v=0.25,\n",
    "        mixup=0.0,\n",
    "        optimizer=\"SGD\",\n",
    "        lr0=0.02,\n",
    "        weight_decay=0.01,\n",
    "        fliplr=0.5,\n",
    "        mosaic=0.25\n",
    "    )\n",
    "    \n",
    "    log_metrics_to_wandb(\n",
    "        results_baseline_p, \n",
    "        run_id=run.id, \n",
    "        project_name=PROJECT_NAME\n",
    "    )\n",
    "    \n",
    "    del model_baseline_p\n",
    "    gc.collect()\n",
    "    empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adf863d",
   "metadata": {},
   "source": [
    "### Result Analysis (The \"Cold Start\" Failure)\n",
    "As hypothesized, the model failed to generalize.\n",
    "* **mAP @ 50-95:** Dropped drastically to **0.4611**.\n",
    "* **Recall:** Only **0.3923**, meaning it missed more than half of the objects.\n",
    "This proves that without pre-trained weights, the dataset is too small for the model to learn feature extraction from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c164f92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline YOLO11p Result on Sample Dataset:\")\n",
    "print_metrics(results_baseline_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8bdfc6",
   "metadata": {},
   "source": [
    "## 6. Transfer Learning Strategy\n",
    "\n",
    "To fix the poor performance of Experiment C, I implement a two-stage training process.\n",
    "\n",
    "### Stage 1: Pretraining on Public Data\n",
    "I train the `yolov11p` architecture on the larger, public Roboflow dataset (generic bottle caps). This stage ignores color classes and focuses purely on **object localization** (learning the shape)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a569b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project=PROJECT_NAME, name=PRETRAIN_P_RUN_NAME) as run:\n",
    "    model_p_pretrain = YOLO(\"yolo11p.yaml\")\n",
    "    \n",
    "    results_p_pretrain = model_p_pretrain.train(\n",
    "        data=str(public_config_path),\n",
    "        project=PROJECT_NAME,\n",
    "        name=PRETRAIN_P_RUN_NAME,\n",
    "        epochs=25,\n",
    "        verbose=True,\n",
    "\n",
    "        hsv_h=0.0,\n",
    "        hsv_s=0.25,\n",
    "        hsv_v=0.0,\n",
    "        mixup=0.0,\n",
    "        optimizer=\"SGD\",\n",
    "        lr0=0.02,\n",
    "        weight_decay=0.01,\n",
    "        fliplr=0.5,\n",
    "        mosaic=0.0\n",
    "    )\n",
    "\n",
    "    log_metrics_to_wandb(\n",
    "        results_p_pretrain, \n",
    "        run_id=run.id, \n",
    "        project_name=PROJECT_NAME\n",
    "    )\n",
    "    \n",
    "    del model_p_pretrain\n",
    "    gc.collect()\n",
    "    empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738122cc",
   "metadata": {},
   "source": [
    "### Result Analysis\n",
    "On the public dataset, the Pico model achieved an **mAP @ 50-95 of 0.8745**. This proves that the tiny \"Pico\" architecture *is* capable of learning complex features when given enough data. We now have a solid \"backbone\" saved as `best.pt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8992fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pretraining Result (Trained on Public Dataset):\")\n",
    "print_metrics(results_p_pretrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67000985",
   "metadata": {},
   "source": [
    "### Stage 2: Fine-Tuning\n",
    "Now I load the weights from Stage 1 (`best.pt`) and fine-tune the model on our specific project dataset to distinguish **Light Blue vs. Dark Blue**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af30ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with wandb.init(project=PROJECT_NAME, name=FINE_TUNE_P_RUN_NAME) as run:\n",
    "    stage1_weights_path = Path(PROJECT_NAME) / PRETRAIN_P_RUN_NAME / \"weights\" / \"best.pt\"\n",
    "\n",
    "    model_p_finetune = YOLO(stage1_weights_path)\n",
    "    \n",
    "    results_p_finetune = model_p_finetune.train(\n",
    "        data=str(sample_config_path),\n",
    "        project=PROJECT_NAME,\n",
    "        name=FINE_TUNE_P_RUN_NAME,\n",
    "        epochs=150,\n",
    "        verbose=True,\n",
    "        hsv_h=0.0,\n",
    "        hsv_s=0.25,\n",
    "        hsv_v=0.25,\n",
    "        mixup=0.0,\n",
    "        optimizer=\"SGD\",\n",
    "        lr0=0.02,\n",
    "        weight_decay=0.01,\n",
    "        fliplr=0.5,\n",
    "        mosaic=0.25\n",
    "    )\n",
    "    \n",
    "    log_metrics_to_wandb(\n",
    "        results_p_finetune, \n",
    "        run_id=run.id, \n",
    "        project_name=PROJECT_NAME\n",
    "    )\n",
    "    \n",
    "    del model_p_finetune\n",
    "    gc.collect()\n",
    "    empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc39fcfe",
   "metadata": {},
   "source": [
    "### Result Analysis\n",
    "The transfer learning strategy was a success.\n",
    "* **mAP @ 50-95:** The model achieved **0.8742**, which is nearly identical to the Nano model (0.90) and far superior to the random-init Pico (0.46).\n",
    "* **Recall:** It achieved a perfect **1.00**, ensuring no bottle caps were missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fine-Tuned YOLOv11p Result (Trained on Sample Dataset):\")\n",
    "print_metrics(results_p_finetune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90e003d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The Two-Stage Transfer Learning approach allowed us to use a highly optimized architecture without sacrificing significant accuracy.\n",
    "\n",
    "![Comparison Baseline and Fine-Tuned YOLO11p](../assets/yolo11p_comparison.png)\n",
    "\n",
    "**Final Comparison:**\n",
    "\n",
    "| Model | Weights | mAP @ 50-95 | Params | Status |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **YOLO11s** | COCO Pretrained | **0.9172** | 9.5M | Too Heavy |\n",
    "| **YOLO11n** | COCO Pretrained | **0.9018** | 2.6M | Baseline |\n",
    "| **YOLO11p** | Random Init | 0.4611 | 1.5M | Failed |\n",
    "| **YOLO11p** | **Pretrained + Finetuned** | **0.8742** | **1.5M** | **Selected** |\n",
    "\n",
    "\n",
    "**Note:** For the full inference speed analysis on the edge hardware, please refer to the `README.md` file in the repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bojongtrash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
